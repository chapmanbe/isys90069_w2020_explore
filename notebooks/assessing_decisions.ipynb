{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Classification Accuracy\n",
    "\n",
    "## [Signal Detection Theory](http://gureckislab.org/courses/fall19/labincp/chapters/10/00-sdt.html)\n",
    "\n",
    "- Why do we need Signal Detection Theory?\n",
    "- What are the roots of signal detection theory?\n",
    "\n",
    "## What is an ROC Curve?\n",
    "- How is it generated?\n",
    "- What are its characteristics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdsutils.mutils import *\n",
    "from cdsutils.sdt import *\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Binary Decisions\n",
    "\n",
    "We are going to explore the simplest decision case: There is a single observable variable, for example, [temperature](https://medlineplus.gov/ency/article/001982.htm) or [white blood count](https://medlineplus.gov/ency/article/003643.htm) that has a continuous value and a binary classification that is being made based on that variable. For example, the patient has __infection__ (which we will refer to as __positive__) or __no infection__ (which we will refer to as __negative__). Further, we are going to assume that greater (more positive) values for the observable variable are associated with __positive disease state__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensitivity and Specificity\n",
    "\n",
    "The __sensitivity__ (true positive fraction (TPF)) of a test is the probability that an actually positive case will be labeled as positive by the test. It can be computed as the number of __actually positive__ cases __labeled__ as positive by the test divided by the __total number of positive cases__.\n",
    "\n",
    "The __specificity__ (true negative fraction (TNF)) of a test is the probability that an actually negative case will be labeled as negative by the test. It can be computed as the number of __actually negative__ cases __labeled__ as negative by the test divided byt he __total number of negative cases__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af48d842e71405487e22512ad26c5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SensSpecCheck(children=(HBox(children=(VBox(children=(Button(description='Check Scores', style=ButtonStyle()),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SensSpecCheck()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "Computing the sensitivity and specificity is easier if we first compute the __confusion matrix__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93fa7d26c364304a3de12f138534ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SensSpecCheck2(children=(HBox(children=(VBox(children=(Button(description='Check Scores', style=ButtonStyle())…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SensSpecCheck2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points for Sensitivity and Specificity\n",
    "\n",
    "- Sensitivity and Specificity are __insensitive__ to the disease prevalence\n",
    "- Sensitivity and specificity depend to the arbitrary choice of the __threshold__.\n",
    "- For an imperfect test, when I change the threshold to __increase sensitivity__ I will necessary __decrease specificity__. Similarly, any threshold change to __increase specificity__ will necessarily __decrease sensitivity__. This will be explored more below with ROC curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPV and NPV\n",
    "\n",
    "The predictive value of a test depends on the prevalence of the disease in the population being tested. The positive predictive value (PPV) and the negative predictive value (NPV) capture this dependency.\n",
    "\n",
    "The PPV is the probability that someone with a positive test is actually disease positive. PPV is computed as the number of actually positive cases with a positive test divided by the total number of cases with a positive test.\n",
    "\n",
    "Similarly the NPV is the probability that someone with a negative test is actually disease negative. NPV is computed as the number of actually negative cases with a negative test value divided by the total number of cases with a negative test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic (ROC) Curves\n",
    "\n",
    "Because sensitivity and specificity are dependent on the arbitrary choice of the threshold, it is often desirable to have a measure that summarizes a test across all possible choices of a threshold. A common way of doing this is with ROC curves, where we can compute the area under the curve (AUC) as a summary statistic of the test's performance. in the cells below, you can explore these ideas with different populations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Generate some random populations\n",
    "\n",
    "In my analyses I assume that __positive__ cases have a more __positive__ value of the test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6546b8b834416088de4276c5cddfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExploreStats(children=(HBox(children=(VBox(children=(Dropdown(description='Number:', options=('uniform', 'gaus…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ExploreStats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
